{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите на ютубе интересную лекцию и запустите на ней нейро-копирайтера. Получите аудио-дорожку, на шаге транскрибации переведите текст на английский язык (для этого изучите документацию whisper - https://github.com/openai/whisper).\n",
    "\n",
    "Сделайте нейро консультанта на базе, который бы отвечал на русском языке, а методичку - на английском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install yt-dlp -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pydub tiktoken openai langchain langchain_community langchain-openai faiss-cpu langchain-text-splitters pytube\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import subprocess\n",
    "import openai\n",
    "from google.colab import userdata\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.docstore.document import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование ключа API от ProxyAPI\n",
    "key = userdata.get('PROXY_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = key\n",
    "\n",
    "# Адрес сервера ProxyAPI\n",
    "base_url = 'https://api.proxyapi.ru/openai/v1'\n",
    "os.environ[\"OPENAI_BASE_URL\"] = base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение аудио-дорожки из видео по ссылке и загрузка в колаб"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "\n",
    "# Ссылка на видео YouTube для скачивания\n",
    "URLS = ['https://youtu.be/W5qZbt5_BtI?si=wAbOg5yIvLzUxeIa']  # указываем ссылку для получения аудио-дорожки из видео\n",
    "\n",
    "# Придумаем короткое имя для сохраняемого аудио файла на английском языке.\n",
    "name = 'My_Audio_2'\n",
    "file_name = f\"download_{name}.m4a\"\n",
    "\n",
    "# Значения для получения лучшего качества звука из документации.\n",
    "ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "    }],\n",
    "    'outtmpl': file_name,\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    error_code = ydl.download(URLS)\n",
    "    if error_code == 0:\n",
    "        print('Загрузка файла прошла успешно!')\n",
    "    else:\n",
    "        print(f'Код ошибки: {error_code}')\n",
    "\n",
    "print(f\"\\nАудиофайл загружен, имя файла: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводим путь до скачанного файла\n",
    "audio_file_path = file_name\n",
    "audio_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация клиента OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def transcribe_audio_whisper_chunked(audio_path, file_title, save_folder_path, max_duration=5 * 60 * 1000):  # 5 минут\n",
    "    \"\"\"\n",
    "    Функция для транскрибации аудиофайла на части, чтобы соответствовать ограничениям размера API.\n",
    "    \"\"\"\n",
    "\n",
    "    # Создание папки для сохранения результатов, если она ещё не существует\n",
    "    os.makedirs(save_folder_path, exist_ok=True)\n",
    "\n",
    "    # Загрузка аудиофайла\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Создание временной папки для хранения аудио чанков (фрагментов)\n",
    "    temp_dir = os.path.join(save_folder_path, \"temp_audio_chunks\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Инициализация переменных для обработки аудио чанков\n",
    "    current_start_time = 0  # Текущее время начала чанка\n",
    "    chunk_index = 1         # Индекс текущего чанка\n",
    "    transcriptions = []     # Список для хранения всех транскрибаций\n",
    "\n",
    "    # Обработка аудиофайла чанками\n",
    "    while current_start_time < len(audio):\n",
    "        # Выделение чанка из аудиофайла\n",
    "        chunk = audio[current_start_time:current_start_time + max_duration]\n",
    "        # Формирование имени и пути файла чанка\n",
    "        chunk_name = f\"chunk_{chunk_index}.wav\"\n",
    "        chunk_path = os.path.join(temp_dir, chunk_name)\n",
    "        # Экспорт чанка в формате wav\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "\n",
    "        # Проверка размера файла чанка на соответствие лимиту API\n",
    "        if os.path.getsize(chunk_path) > 26214400:  # 25 MB\n",
    "            print(f\"Chunk {chunk_index} exceeds the maximum size limit for the API. Trying a smaller duration...\")\n",
    "            max_duration = int(max_duration * 0.9)  # Уменьшение длительности чанка на 10%\n",
    "            os.remove(chunk_path)  # Удаление чанка, превышающего лимит\n",
    "            continue\n",
    "\n",
    "        # Открытие файла чанка для чтения в двоичном режиме\n",
    "        with open(chunk_path, \"rb\") as src_file:\n",
    "            print(f\"Transcribing {chunk_name}...\")\n",
    "            try:\n",
    "                # Запрос на транскрибацию чанка с использованием модели Whisper\n",
    "                transcript_response = client.audio.translations.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=src_file\n",
    "                )\n",
    "                # Добавление результата транскрибации в список транскрипций\n",
    "                transcriptions.append(transcript_response.text)\n",
    "            except openai.BadRequestError as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "        # Удаление обработанного файла чанка\n",
    "        os.remove(chunk_path)\n",
    "        # Переход к следующему чанку\n",
    "        current_start_time += max_duration\n",
    "        chunk_index += 1\n",
    "\n",
    "    # Удаление временной папки с чанками\n",
    "    os.rmdir(temp_dir)\n",
    "\n",
    "    # Сохранение всех транскрипций в один текстовый файл\n",
    "    result_path = os.path.join(save_folder_path, f\"{file_title}.txt\")\n",
    "    with open(result_path, \"w\") as txt_file:\n",
    "        txt_file.write(\"\\n\".join(transcriptions))\n",
    "\n",
    "    print(f\"Transcription saved to {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры файла и вызов функции\n",
    "audio_path = '/content/download_My_Audio_2.m4a'\n",
    "file_title = 'What_is_CORS_How_to_link_the_frontend_and_backend'\n",
    "save_folder_path = '/content/transcriptions/'\n",
    "\n",
    "res = transcribe_audio_whisper_chunked(audio_path, file_title, save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Разделяем текст на логические блоки с выделением названия раздела:\n",
    "system = '\\u0412\\u044B \\u0433\\u0435\\u043D\\u0438\\u0439 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0430, \\u043A\\u043E\\u043F\\u0438\\u0440\\u0430\\u0439\\u0442\\u0438\\u043D\\u0433\\u0430, \\u043F\\u0438\\u0441\\u0430\\u0442\\u0435\\u043B\\u044C\\u0441\\u0442\\u0432\\u0430. \\u0412\\u0430\\u0448\\u0430 \\u0437\\u0430\\u0434\\u0430\\u0447\\u0430 \\u0440\\u0430\\u0441\\u043F\\u043E\\u0437\\u043D\\u0430\\u0442\\u044C \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u044B \\u0432 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435 \\u0438 \\u0440\\u0430\\u0437\\u0431\\u0438\\u0442\\u044C \\u0435\\u0433\\u043E \\u043D\\u0430 \\u044D\\u0442\\u0438 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u044B \\u0441\\u043E\\u0445\\u0440\\u0430\\u043D\\u044F\\u044F \\u0432\\u0435\\u0441\\u044C \\u0442\\u0435\\u043A\\u0441\\u0442 \\u043D\\u0430 100%' #@param {type:\"string\"}\n",
    "user = '\\u041F\\u043E\\u0436\\u0430\\u043B\\u0443\\u0439\\u0441\\u0442\\u0430, \\u0434\\u0430\\u0432\\u0430\\u0439\\u0442\\u0435 \\u043F\\u043E\\u0434\\u0443\\u043C\\u0430\\u0435\\u043C \\u0448\\u0430\\u0433 \\u0437\\u0430 \\u0448\\u0430\\u0433\\u043E\\u043C: \\u041F\\u043E\\u0434\\u0443\\u043C\\u0430\\u0439\\u0442\\u0435, \\u043A\\u0430\\u043A\\u0438\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u044B \\u0432 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435 \\u0432\\u044B \\u043C\\u043E\\u0436\\u0435\\u0442\\u0435 \\u0440\\u0430\\u0441\\u043F\\u043E\\u0437\\u043D\\u0430\\u0442\\u044C \\u0438 \\u043A\\u0430\\u043A\\u043E\\u0435 \\u043D\\u0430\\u0437\\u0432\\u0430\\u043D\\u0438\\u0435 \\u043F\\u043E \\u0441\\u043C\\u044B\\u0441\\u043B\\u0443 \\u043C\\u043E\\u0436\\u043D\\u043E \\u0434\\u0430\\u0442\\u044C \\u043A\\u0430\\u0436\\u0434\\u043E\\u043C\\u0443 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0443. \\u0414\\u0430\\u043B\\u0435\\u0435 \\u043D\\u0430\\u043F\\u0438\\u0448\\u0438\\u0442\\u0435 \\u043E\\u0442\\u0432\\u0435\\u0442 \\u043F\\u043E \\u0432\\u0441\\u0435\\u043C\\u0443 \\u043F\\u0440\\u0435\\u0434\\u044B\\u0434\\u0443\\u0449\\u0435\\u043C\\u0443 \\u043E\\u0442\\u0432\\u0435\\u0442\\u0443 \\u0432 \\u043F\\u043E\\u0440\\u044F\\u0434\\u043A\\u0435: ## \\u041D\\u0430\\u0437\\u0432\\u0430\\u043D\\u0438\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0430, \\u043F\\u043E\\u0441\\u043B\\u0435 \\u0447\\u0435\\u0433\\u043E \\u0432\\u0435\\u0441\\u044C \\u0442\\u0435\\u043A\\u0441\\u0442, \\u043E\\u0442\\u043D\\u043E\\u0441\\u044F\\u0449\\u0438\\u0439\\u0441\\u044F \\u043A \\u044D\\u0442\\u043E\\u043C\\u0443 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0443. \\u0422\\u0435\\u043A\\u0441\\u0442:' #@param {type:\"string\"}\n",
    "\n",
    "temperature = 0 #@param {type: \"slider\", min: 0, max: 1, step:0.1}\n",
    "chunk_size = 7000 #@param {type: \"slider\", min: 1000, max: 7000, step:500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Функции\n",
    "# Функция настройки стиля для переноса текста в выводе ячеек\n",
    "# для изменения стиля отображения текста, так чтобы предотвратить переполнение текста за границы ячейки вывода и обеспечить его перенос.\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def set_text_wrap_css():\n",
    "    css = '''\n",
    "    <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "    </style>\n",
    "    '''\n",
    "    display(HTML(css))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_text_wrap_css)\n",
    "\n",
    "# Функция подсчета количества токенов\n",
    "def num_tokens_from_messages(messages, model='gpt-4o-mini'):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "    if model in ['gpt-4o-mini']:\n",
    "        num_tokens = 0\n",
    "\n",
    "        for message in messages:\n",
    "            num_tokens += 4\n",
    "\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "\n",
    "                if key == 'name':\n",
    "                    num_tokens -= 1\n",
    "\n",
    "        num_tokens += 2\n",
    "        return num_tokens\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f'''num_tokens_from_messages() is not presently implemented for model {model}.''')\n",
    "\n",
    "\n",
    "# Функция дробления текста на чанки\n",
    "def split_text(txt_file, chunk_size=chunk_size):\n",
    "    source_chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(separators=['\\n', '\\n\\n', '. '], chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "    for chunk in splitter.split_text(txt_file):\n",
    "        source_chunks.append(Document(page_content=chunk, metadata={\"meta\":\"data\"}))\n",
    "\n",
    "    print(f'\\n\\nТекст разбит на {len(source_chunks)} чанков.')\n",
    "\n",
    "    return source_chunks\n",
    "\n",
    "\n",
    "# Функция получения ответа от модели\n",
    "def answer_index(system, user, chunk, temp=temperature, model='gpt-4o-mini'):\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system},\n",
    "        {'role': 'user', 'content': user + f'{chunk}'}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temp\n",
    "    )\n",
    "\n",
    "    # Вывод количества токенов отключен\n",
    "    # print(f'\\n====================\\n\\n{num_tokens_from_messages(messages)} токенов будет использовано на чанк\\n\\n')\n",
    "    answer = completion.choices[0].message.content\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def process_one_file(file_path, system, user):\n",
    "    with open(file_path, 'r') as txt_file:\n",
    "        text = txt_file.read()\n",
    "    source_chunks = split_text(text)\n",
    "    processed_text = ''\n",
    "    unprocessed_text = ''\n",
    "\n",
    "    for chunk in source_chunks:\n",
    "        attempt = 0\n",
    "\n",
    "        while attempt < 3:\n",
    "            try:\n",
    "                answer = answer_index(system, user, chunk.page_content)\n",
    "                break  # Успешно получили ответ, выходим из цикла попыток\n",
    "\n",
    "            except Exception as e:\n",
    "                attempt += 1  # Увеличиваем счетчик попыток\n",
    "                print(f'\\n\\nПопытка {attempt} не удалась из-за ошибки: {str(e)}')\n",
    "                time.sleep(10)  # Ожидаем перед следующей попыткой\n",
    "                if attempt == 3:\n",
    "                    answer = ''\n",
    "                    print(f'\\n\\nОбработка элемента {chunk} не удалась после 3 попыток')\n",
    "                    unprocessed_text += f'{chunk}\\n\\n'\n",
    "\n",
    "        processed_text += f'{answer}\\n\\n'  # Добавляем ответ в результат\n",
    "        print(f'{answer}')  # Выводим ответ\n",
    "\n",
    "    return processed_text, unprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Запуск\n",
    "file_path = '/content/transcriptions/What_is_CORS_How_to_link_the_frontend_and_backend.txt'\n",
    "#file_path = '/content/transcriptions/Графы_Алгоритмы_и_Структуры_Данных_на_Python.txt'\n",
    "# Вызываем функцию обработки для этого файла\n",
    "processed_text, unprocessed_text = process_one_file(file_path, system, user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем заголовки, на которые следует разбить текст\n",
    "headers_to_split_on = [\n",
    "    (\"##\", \"Header 2\")\n",
    "    ]\n",
    "# Создаем объект для разбиения текста на секции по заголовкам\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Получаем список документов, разбитых по заголовкам\n",
    "md_header_splits = markdown_splitter.split_text(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращаем заголовок из метаданных в текст чанка\n",
    "# Проходим по каждому документу в списке\n",
    "for document in md_header_splits:\n",
    "    # Извлекаем заголовок из метаданных документа\n",
    "    # заголовок хранится под ключом 'Header 2'\n",
    "    header = document.metadata['Header 2']\n",
    "\n",
    "    # Добавляем заголовок к содержимому документа, разделяя точкой и пробелом\n",
    "    # Если необходимо добавить заголовок именно в начало содержимого, используйте следующий формат\n",
    "    document.page_content = f'{header}. {document.page_content}'\n",
    "\n",
    "# После выполнения цикла, md_header_splits будет содержать обновлённые документы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Обрабатываем каждый чанк, выделяя только суть для методички\n",
    "system = \"\\u0412\\u044B \\u0433\\u0435\\u043D\\u0438\\u0439 \\u043A\\u043E\\u043F\\u0438\\u0440\\u0430\\u0439\\u0442\\u0438\\u043D\\u0433\\u0430, \\u044D\\u043A\\u0441\\u043F\\u0435\\u0440\\u0442 \\u0432 \\u043F\\u0440\\u043E\\u0433\\u0440\\u0430\\u043C\\u043C\\u0438\\u0440\\u043E\\u0432\\u0430\\u043D\\u0438\\u0438 \\u043D\\u0430 \\u043F\\u0430\\u0439\\u0442\\u043E\\u043D \\u0438 \\u0432 \\u0442\\u0435\\u043C\\u0435 \\\"\\u0413\\u0440\\u0430\\u0444\\u044B, \\u0430\\u043B\\u0433\\u043E\\u0440\\u0438\\u0442\\u043C\\u044B \\u0438 \\u0441\\u0442\\u0440\\u0443\\u043A\\u0442\\u0443\\u0440\\u044B \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445\\\". \\u0412\\u044B \\u043F\\u043E\\u043B\\u0443\\u0447\\u0430\\u0435\\u0442\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B \\u043D\\u0435\\u043E\\u0431\\u0440\\u0430\\u0431\\u043E\\u0442\\u0430\\u043D\\u043D\\u043E\\u0433\\u043E \\u0442\\u0435\\u043A\\u0441\\u0442\\u0430 \\u043F\\u043E \\u043E\\u043F\\u0440\\u0435\\u0434\\u0435\\u043B\\u0435\\u043D\\u043D\\u043E\\u0439 \\u0442\\u0435\\u043C\\u0435. \\u041D\\u0443\\u0436\\u043D\\u043E \\u0438\\u0437 \\u044D\\u0442\\u043E\\u0433\\u043E \\u0442\\u0435\\u043A\\u0441\\u0442\\u0430 \\u0432\\u044B\\u0434\\u0435\\u043B\\u0438\\u0442\\u044C \\u0441\\u0430\\u043C\\u0443\\u044E \\u0441\\u0443\\u0442\\u044C, \\u0442\\u043E\\u043B\\u044C\\u043A\\u043E \\u0441\\u0430\\u043C\\u043E\\u0435 \\u0432\\u0430\\u0436\\u043D\\u043E\\u0435, \\u0441\\u043E\\u0445\\u0440\\u0430\\u043D\\u0438\\u0432 \\u0432\\u0441\\u0435 \\u043D\\u0443\\u0436\\u043D\\u044B\\u0435 \\u043F\\u043E\\u0434\\u0440\\u043E\\u0431\\u043D\\u043E\\u0441\\u0442\\u0438 \\u0438 \\u0434\\u0435\\u0442\\u0430\\u043B\\u0438, \\u043D\\u043E \\u0443\\u0431\\u0440\\u0430\\u0432 \\u0432\\u0441\\u044E \\\"\\u0432\\u043E\\u0434\\u0443\\\" \\u0438 \\u0441\\u043B\\u043E\\u0432\\u0430 (\\u043F\\u0440\\u0435\\u0434\\u043B\\u043E\\u0436\\u0435\\u043D\\u0438\\u044F), \\u043D\\u0435 \\u043D\\u0435\\u0441\\u0443\\u0449\\u0438\\u0435 \\u0441\\u043C\\u044B\\u0441\\u043B\\u043E\\u0432\\u043E\\u0439 \\u043D\\u0430\\u0433\\u0440\\u0443\\u0437\\u043A\\u0438.\" #@param {type:\"string\"}\n",
    "user = \"\\u0418\\u0437 \\u0434\\u0430\\u043D\\u043D\\u043E\\u0433\\u043E \\u0442\\u0435\\u043A\\u0441\\u0442\\u0430 \\u0432\\u044B\\u0434\\u0435\\u043B\\u0438 \\u0442\\u043E\\u043B\\u044C\\u043A\\u043E \\u0446\\u0435\\u043D\\u043D\\u0443\\u044E \\u0441 \\u0442\\u043E\\u0447\\u043A\\u0438 \\u0437\\u0440\\u0435\\u043D\\u0438\\u044F \\u0442\\u0435\\u043C\\u044B \\\"\\u0433\\u0440\\u0430\\u0444\\u044B, \\u0430\\u043B\\u0433\\u043E\\u0440\\u0438\\u0442\\u043C\\u044B \\u0438 \\u0441\\u0442\\u0440\\u0443\\u043A\\u0442\\u0443\\u0440\\u044B \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445\\\" \\u0438\\u043D\\u0444\\u043E\\u0440\\u043C\\u0430\\u0446\\u0438\\u044E. \\u0423\\u0434\\u0430\\u043B\\u0438 \\u0432\\u0441\\u044E \\\"\\u0432\\u043E\\u0434\\u0443\\\". \\u0412 \\u0438\\u0442\\u043E\\u0433\\u0435 \\u0443 \\u0442\\u0435\\u0431\\u044F \\u0434\\u043E\\u043B\\u0436\\u0435\\u043D \\u043F\\u043E\\u043B\\u0443\\u0447\\u0438\\u0442\\u0441\\u044F \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B \\u0434\\u043B\\u044F \\u043C\\u0435\\u0442\\u043E\\u0434\\u0438\\u0447\\u043A\\u0438 \\u043F\\u043E \\u0443\\u043A\\u0430\\u0437\\u0430\\u043D\\u043D\\u043E\\u0439 \\u0442\\u0435\\u043C\\u0435. \\u041E\\u043F\\u0438\\u0440\\u0430\\u0439\\u0441\\u044F \\u0442\\u043E\\u043B\\u044C\\u043A\\u043E \\u043D\\u0430 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0439 \\u0442\\u0435\\u0431\\u0435 \\u0442\\u0435\\u043A\\u0441\\u0442, \\u043D\\u0435 \\u043F\\u0440\\u0438\\u0434\\u0443\\u043C\\u044B\\u0432\\u0430\\u0439 \\u043D\\u0438\\u0447\\u0435\\u0433\\u043E \\u043E\\u0442 \\u0441\\u0435\\u0431\\u044F. \\u041E\\u0442\\u0432\\u0435\\u0442 \\u043D\\u0443\\u0436\\u0435\\u043D \\u0432 \\u0444\\u043E\\u0440\\u043C\\u0430\\u0442\\u0435 ## \\u041D\\u0430\\u0437\\u0432\\u0430\\u043D\\u0438\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0430, \\u0438 \\u0434\\u0430\\u043B\\u0435\\u0435 \\u0432\\u044B\\u0434\\u0435\\u043B\\u0435\\u043D\\u043D\\u0430\\u044F \\u0442\\u043E\\u0431\\u043E\\u0439 \\u0446\\u0435\\u043D\\u043D\\u0430\\u044F \\u0438\\u043D\\u0444\\u043E\\u0440\\u043C\\u0430\\u0446\\u0438\\u044F \\u0438\\u0437 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0430. \\u0415\\u0441\\u043B\\u0438 \\u0432 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435 \\u043D\\u0435 \\u0441\\u043E\\u0434\\u0435\\u0440\\u0436\\u0438\\u0442\\u0441\\u044F \\u0446\\u0435\\u043D\\u043D\\u043E\\u0439 \\u0438\\u043D\\u0444\\u043E\\u0440\\u043C\\u0430\\u0446\\u0438\\u0438, \\u0442\\u043E \\u043E\\u0441\\u0442\\u0430\\u0432\\u044C \\u0442\\u043E\\u043B\\u044C\\u043A\\u043E  \\u043D\\u0430\\u0437\\u0432\\u0430\\u043D\\u0438\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0430, \\u043D\\u0430\\u043F\\u0440\\u0438\\u043C\\u0435\\u0440: \\\"## \\u0412\\u0432\\u0435\\u0434\\u0435\\u043D\\u0438\\u0435\\\". \\u0422\\u0435\\u043A\\u0441\\u0442:\" #@param {type:\"string\"}\n",
    "\n",
    "temperature = 0 #@param {type: \"slider\", min: 0, max: 1, step:0.1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents, system, user, temperature):\n",
    "    \"\"\"\n",
    "    Функция принимает чанки, system, user, temperature для модели.\n",
    "    Она обрабатывает каждый документ, используя модель GPT, конкатенирует результаты в один текст и сохраняет в файл .txt.\n",
    "    В итоге мы получаем методичку по лекции.\n",
    "    \"\"\"\n",
    "    processed_text_for_handbook = \"\"  # Строка для конкатенации обработанного текста\n",
    "\n",
    "    for document in documents:\n",
    "        # Получаем ответ от модели\n",
    "        answer = answer_index(system, user, document.page_content, temperature, model='gpt-4o')\n",
    "        # Добавляем обработанный текст в общую строку\n",
    "        processed_text_for_handbook += f\"{answer}\\n\\n\"\n",
    "\n",
    "    # Записываем полученный текст в файл\n",
    "    with open('processed_documents.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(processed_text_for_handbook)\n",
    "\n",
    "    # Функция возвращает путь к файлу с обработанным текстом\n",
    "    return 'processed_documents.txt'\n",
    "\n",
    "# Применение функции\n",
    "file_path = process_documents(md_header_splits, system, user, temperature)\n",
    "print(f\"Обработанный текст сохранен в файле: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение и вывод содержимого методички:\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    processed_text = f.read()\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание нейро-консультанта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализирум модель эмбеддингов\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Создадим индексную базу из разделенных фрагментов текста\n",
    "db = FAISS.from_documents(md_header_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_for_NA = \"\"\"Ты - преподаватель, эксперт по теме ''\n",
    "                  Твоя задача - ответить студенту на вопрос только на основе представленных тебе документов, не добавляя ничего от себя. А также ты должен отвечать на русском языке\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_neuro_assist(system, topic, search_index, verbose=1):\n",
    "\n",
    "    # Поиск релевантных отрезков из базы знаний\n",
    "    docs = search_index.similarity_search(topic, k=3)\n",
    "    if verbose: print('\\n ===========================================: ')\n",
    "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
    "    if verbose: print('message_content :\\n ======================================== \\n', message_content)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_for_NA},\n",
    "        {\"role\": \"user\", \"content\": f\"Ответь на вопрос студента. Не упоминай отрывки документов с информацией для ответа студенту в ответе. Ответ должен быть на русском языке. Документ с информацией для ответа студенту: {message_content}\\n\\nВопрос студента: \\n{topic}\"}\n",
    "    ]\n",
    "\n",
    "    if verbose: print('\\n ===========================================: ')\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    answer = completion.choices[0].message.content\n",
    "    return answer  # возвращает ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"Что такое CORS?\"\n",
    "ans=answer_neuro_assist(system, topic, db, verbose=1)\n",
    "ans"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
